{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# print date as date accessed\n",
    "date_accessed = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"Date accessed: {date_accessed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import dask\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import os, sys, glob, re, time, math, calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.distributed as dd\n",
    "if 'client' in locals():\n",
    "    client.close()\n",
    "    cluster.close()\n",
    "cluster = dd.LocalCluster(n_workers=12, dashboard_address=8787)\n",
    "client = dd.Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "files = sorted(glob.glob(f'/data/harish/Estimation-of-lidar-wind-speed-profiles-from-ERA5-inputs-using-TabNet/data/{year}/PRES*'))\n",
    "ds = xr.open_mfdataset(files,combine='nested', concat_dim='valid_time', parallel=True,\n",
    "                                chunks={'pressure_level': -1,'latitude': -1, 'longitude': -1, 'valid_time': -1})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2020\n",
    "files = sorted(glob.glob(f'/data/harish/Estimation-of-lidar-wind-speed-profiles-from-ERA5-inputs-using-TabNet/data/{year}/SFC*'))\n",
    "ds = xr.open_mfdataset(files,combine='nested', concat_dim='valid_time', parallel=True,\n",
    "                                chunks={'pressure_level': -1,'latitude': -1, 'longitude': -1, 'valid_time': -1})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find the closest index in a 1D array\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def era5_pres_hourly(year, par, level, target_lat,target_lon,location):\n",
    "    files = sorted(glob.glob(f'/data/harish/Estimation-of-lidar-wind-speed-profiles-from-ERA5-inputs-using-TabNet/data/{year}/PRES*'))\n",
    "    hr_data = xr.open_mfdataset(files,combine='nested', concat_dim='valid_time', parallel=True,\n",
    "                                chunks={'pressure_level': -1,'latitude': -1, 'longitude': -1, 'valid_time': -1})\n",
    "    # remiving unnecessary multiple dimensions names\n",
    "    hr_data = hr_data.drop_vars(['number','expver'])\n",
    "\n",
    "    hr_cor_data = hr_data[par].sel(pressure_level=level).sel(latitude=target_lat, \n",
    "                      longitude=target_lon, method='nearest').drop_vars('pressure_level')\n",
    "    hr_cor_data['location'] = location\n",
    "    hr_cor_data = hr_cor_data.rename(f'{par}_{level}')\n",
    "    return hr_cor_data\n",
    "\n",
    "def era5_sfc_hourly(year,par,target_lat,target_lon,location):\n",
    "    files = sorted(glob.glob(f'/data/harish/Estimation-of-lidar-wind-speed-profiles-from-ERA5-inputs-using-TabNet/data/{year}/SFC*'))\n",
    "    hr_data = xr.open_mfdataset(files,combine='nested', concat_dim='valid_time', parallel=True,\n",
    "                                chunks={'pressure_level': -1,'latitude': -1, 'longitude': -1, 'valid_time': -1})\n",
    "    # remiving unnecessary multiple dimensions names\n",
    "    hr_data = hr_data.drop_vars(['number','expver'])\n",
    "    hr_cor_data = hr_data[par].sel(latitude=target_lat, \n",
    "                      longitude=target_lon, method='nearest')\n",
    "    hr_cor_data['location'] = location\n",
    "    return hr_cor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler_stations = pd.read_csv('data/profiler_locations.csv',usecols=[0,3,4])\n",
    "profiler_stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting surface variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par_names = ['u10', 'v10', 'u100', 'v100','zust','i10fg',\n",
    "            't2m','skt','stl1','d2m','msl','blh','cbh',\n",
    "            'ishf','ie','tcc','lcc','cape','cin','bld']\n",
    "for par_name in (par_names):\n",
    "    # create a folder with name par inside data_dir\n",
    "    par_dir = f'data/ERA5_variables/{par_name}'\n",
    "    os.makedirs(par_dir, exist_ok=True)\n",
    "    # --- extract data at each year ---#\n",
    "    for year in np.arange(2018,2019+1):\n",
    "        datasets = []\n",
    "        for loc in range(len(profiler_stations)):\n",
    "            ds = era5_sfc_hourly(year, par_name, \n",
    "                                 profiler_stations['lat [degrees]'][loc], \n",
    "                                 profiler_stations['lon [degrees]'][loc],\n",
    "                                 profiler_stations['stid'][loc])\n",
    "            datasets.append(ds.compute())\n",
    "            del(ds)\n",
    "        # Concatenate datasets along a new dimension ('location')\n",
    "        combined_dataset = xr.concat(datasets, dim='location')\n",
    "        combined_dataset['year'] = year\n",
    "        file_path = f'{par_dir}/{year}.nc'\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "        combined_dataset.to_netcdf(file_path)\n",
    "        del(combined_dataset)\n",
    "        print(par_name,year)\n",
    "    # --- combining all years data ---#\n",
    "    ds = xr.open_mfdataset(f'{par_dir}/*.nc', \n",
    "                                parallel=True)\n",
    "    file_path = f'data/ERA5_variables/{par_name}.nc'\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "    ds.to_netcdf(file_path)\n",
    "    print(par_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting pressure level variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvls = [1000,975,950]\n",
    "pars = ['u','v','t']\n",
    "pars = ['t']\n",
    "for par in pars:\n",
    "    for level in lvls:\n",
    "        # create a folder with name par and level inside data_dir\n",
    "        par_dir = f'data/ERA5_variables/{par}_{level}'\n",
    "        os.makedirs(par_dir, exist_ok=True)\n",
    "        # --- extract data at each year ---#\n",
    "        for year in np.arange(2020,2023+1):\n",
    "            datasets = []\n",
    "            for loc in range(len(profiler_stations)):\n",
    "                ds = era5_pres_hourly(year, par,level, \n",
    "                                      profiler_stations['lat [degrees]'][loc], \n",
    "                                 profiler_stations['lon [degrees]'][loc],\n",
    "                                 profiler_stations['stid'][loc])\n",
    "                datasets.append(ds.compute())\n",
    "                del(ds)\n",
    "            # Concatenate datasets along a new dimension ('location')\n",
    "            combined_dataset = xr.concat(datasets, dim='location')\n",
    "            combined_dataset['year'] = year\n",
    "            file_path = f'{par_dir}/{year}.nc'\n",
    "            if os.path.exists(file_path):\n",
    "                os.remove(file_path)\n",
    "            combined_dataset.to_netcdf(file_path)\n",
    "            del(combined_dataset)\n",
    "            print(par,level,year)\n",
    "        # --- combining all years data ---#\n",
    "        ds = xr.open_mfdataset(f'{par_dir}/*.nc',\n",
    "                                    parallel=True)\n",
    "        file_path = f'data/ERA5_variables/{par}_{level}.nc'\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "        ds.to_netcdf(file_path)\n",
    "        print(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TabNet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
